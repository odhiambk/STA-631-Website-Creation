{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-16T16:56:54-04:00"
    },
    {
      "path": "index.html",
      "title": "Karlmarx Odhiambo Portfolio",
      "description": "Welcome to my website. I hope you enjoy it!\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-16T16:56:54-04:00"
    },
    {
      "path": "MLR.html",
      "title": "Multiple Linear Regression",
      "author": [],
      "contents": "\r\nIn the last activity, you might have noticed that I made this Rmd output a document with a type of github_document (in the YAML header underneath the title - on line 3) instead of a HTML, Word, or PDF document.\r\nThis produces a GitHub friendly Markdown file that GitHub then renders to HTML.\r\nYou can read more about this output type in RMarkdown’s documentation page if you want to learn more.\r\nDay 1\r\nLoad the necessary packages\r\nI encourage you to continue using the two packages from Posit (formerly RStudio): {tidyverse} and {tidymodels}.\r\nRemember that Emil Hvitfeldt (of Posit) has put together a complementary online text for the labs in the ISLR text that utilize {tidyverse} and {tidymodels} instead of base R.\r\nIn the Packages pane of RStudio, check if {tidyverse} and {tidymodels} are installed.\r\nBe sure to check both your User Library and System Library.\r\nIf either of these are not currently listed (they should be because you verified this in Activity 1), type the following in your Console pane, replacing package_name with the appropriate name, and press Enter/Return afterwards.\r\n\r\n\r\ninstall.packages(\"package_name\")\r\n\r\n\r\nOnce you have verified that both {tidyverse} and {tidymodels} are installed (in either your user or system library), load these packages in the R chunk below titled load-packages.\r\nRun the load-packages code chunk or knit  icon your Rmd document to verify that no errors occur.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(GGally)\r\n\r\n\r\nSince we will be looking at many relationships graphically, it will be nice to not have to code each of these individually.\r\n{GGally} is an extension to {ggplot2} that reduces some of the complexities when combining multiple plots.\r\nFor example, GGally::ggpairs is very handy for pairwise comparisons of multiple variables.\r\nIn the Packages pane of RStudio, check if {GGally} is already installed.\r\nBe sure to check both your User Library and System Library.\r\nIf this is not currently listed, type the following in your Console pane and press Enter/Return afterwards.\r\n\r\n\r\ninstall.packages(\"GGally\")\r\n\r\n\r\nOnce you have verified that {GGally} is installed, load it in the R chunk titled load-packages.\r\nRun the setup code chunk or knit  icon your Rmd document to verify that no errors occur.\r\nLoad the data and\r\nI found a way to upload data from OpenIntro without needing to download it first!\r\nRecall that data we are working with is from the OpenIntro site (its “about” page: https://www.openintro.org/data/index.php?data=hfi).\r\nWe can access the raw data from their tab-delimited text file link: https://www.openintro.org/data/tab-delimited/hfi.txt.\r\nCreate a new R code chunk below that is titled load-data and reads in the above linked TSV (tab-separated values) file by doing the following:\r\n\r\n\r\nhfi_data <- read_csv(\"https://www.openintro.org/data/tab-delimited/hfi.txt\")\r\n\r\n\r\nRather than downloading this file, uploading to RStudio, then reading it in, explore how to load this file directly from the provided URL with readr::read_tsv ({readr} is part of {tidyverse}).\r\nAssign this data set into a data frame named hfi (short for “Human Freedom Index”).\r\nFilter the data hfi data frame for year 2016 and assigns the result to an R data object named hfi_2016. You will use hfi_2016 for the remainder of this activity.\r\nWe will continue using personal freedom scores, pf_score, as the response variable and build on our model that had pf_expression_control as the explanatory variable.\r\nCreate a new R code chunk below, with an appropriate title, that does the following:\r\nReview the about page of the data set and select at least one additional numeric variables (hint: look for <dbl> or <int> designations) to describe its distribution. Remember to write your description.\r\nYou may also wish to do this for pf_score and pf_expression_control again to help you remember what you noticed last week.\r\n\r\n\r\n# Load the data\r\nhfi_data <- read_tsv(\"https://www.openintro.org/data/tab-delimited/hfi.txt\")\r\n\r\n# Filter the data for year 2016\r\nhfi_2016 <- hfi_data %>% filter(year == 2016)\r\n\r\n# Explore the distribution of pf_score\r\nskimr::skim(hfi_2016$pf_score)\r\n\r\nTable 1: Data summary\r\nName\r\nhfi_2016$pf_score\r\nNumber of rows\r\n162\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ndata\r\n0\r\n1\r\n6.98\r\n1.49\r\n2.17\r\n6.03\r\n6.93\r\n8.14\r\n9.4\r\n▁▁▇▇▆\r\n\r\n# Explore the distribution of the selected variable\r\nskimr::skim(hfi_2016$hfi_data)\r\n\r\n# A tibble: 0 × 2\r\n# ℹ 2 variables: skim_type <chr>, skimmed <???>\r\n\r\n\r\n\r\n# Filter the data for year 2016\r\nhfi_2016 <- hfi_data %>% filter(year == 2016)\r\n\r\n# Explore the distribution of pf_score\r\nskimr::skim(hfi_2016$pf_score)\r\n\r\nTable 2: Data summary\r\nName\r\nhfi_2016$pf_score\r\nNumber of rows\r\n162\r\nNumber of columns\r\n1\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nnumeric\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ndata\r\n0\r\n1\r\n6.98\r\n1.49\r\n2.17\r\n6.03\r\n6.93\r\n8.14\r\n9.4\r\n▁▁▇▇▆\r\n\r\n\r\n\r\n# Explore the distribution of the selected variable\r\nskimr::skim(hfi_2016$selected_variable)\r\n\r\n# A tibble: 0 × 2\r\n# ℹ 2 variables: skim_type <chr>, skimmed <???>\r\n\r\nPairwise relationships\r\nIn Activity 2 you explored simple linear regression models.\r\nSpecifically, you fit and assessed this relationship:\r\n\\[\r\ny = \\beta_0 + \\beta_1 \\times x + \\varepsilon\r\n\\]\r\nCheck in\r\nReview how you described this model in Activity 2.\r\n- What were your parameter estimates (i.e., the \\(\\beta\\)s)?\r\nHow did you interpret these and what did they imply for this scenario?\r\n- How good of a fit was this model?\r\nWhat did you use to assess this?\r\nFor this activity, we will begin using the two other quantitative variables to describe the patterns in the response variable.\r\nTake a moment to think about what this previous sentence means:\r\nWhat does this mean from a statistical point of view?\r\nWhat does this mean from a “real world” point of view (i.e., for your data’s situation)?\r\nNow, we will obtain graphical and numerical summaries to describe the pairwise relationships.\r\nIn the code chunk below titled pairs-plot, replace “verbatim” with “r” just before the code chunk title.\r\nReplace explanatory in the select line with the variable you identified above\r\nRun your code chunk or knit your document.\r\n\r\n\r\nstr(hfi_data)\r\n\r\nspc_tbl_ [1,458 × 123] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ year                              : num [1:1458] 2016 2016 2016 2016 2016 ...\r\n $ ISO_code                          : chr [1:1458] \"ALB\" \"DZA\" \"AGO\" \"ARG\" ...\r\n $ countries                         : chr [1:1458] \"Albania\" \"Algeria\" \"Angola\" \"Argentina\" ...\r\n $ region                            : chr [1:1458] \"Eastern Europe\" \"Middle East & North Africa\" \"Sub-Saharan Africa\" \"Latin America & the Caribbean\" ...\r\n $ pf_rol_procedural                 : num [1:1458] 6.66 NA NA 7.1 NA ...\r\n $ pf_rol_civil                      : num [1:1458] 4.55 NA NA 5.79 NA ...\r\n $ pf_rol_criminal                   : num [1:1458] 4.67 NA NA 4.34 NA ...\r\n $ pf_rol                            : num [1:1458] 5.29 3.82 3.45 5.74 5 ...\r\n $ pf_ss_homicide                    : num [1:1458] 8.92 9.46 8.06 7.62 8.81 ...\r\n $ pf_ss_disappearances_disap        : num [1:1458] 10 10 5 10 10 10 10 10 10 10 ...\r\n $ pf_ss_disappearances_violent      : num [1:1458] 10 9.29 10 10 10 ...\r\n $ pf_ss_disappearances_organized    : num [1:1458] 10 5 7.5 7.5 7.5 10 10 7.5 NA 2.5 ...\r\n $ pf_ss_disappearances_fatalities   : num [1:1458] 10 9.93 10 10 9.32 ...\r\n $ pf_ss_disappearances_injuries     : num [1:1458] 10 9.99 10 9.99 9.93 ...\r\n $ pf_ss_disappearances              : num [1:1458] 10 8.84 8.5 9.5 9.35 ...\r\n $ pf_ss_women_fgm                   : num [1:1458] 10 10 10 10 10 10 10 10 NA 10 ...\r\n $ pf_ss_women_missing               : num [1:1458] 7.5 7.5 10 10 5 10 10 7.5 NA 7.5 ...\r\n $ pf_ss_women_inheritance_widows    : num [1:1458] 5 0 5 10 10 10 10 5 NA 0 ...\r\n $ pf_ss_women_inheritance_daughters : num [1:1458] 5 0 5 10 10 10 10 10 NA 0 ...\r\n $ pf_ss_women_inheritance           : num [1:1458] 5 0 5 10 10 10 10 7.5 NA 0 ...\r\n $ pf_ss_women                       : num [1:1458] 7.5 5.83 8.33 10 8.33 ...\r\n $ pf_ss                             : num [1:1458] 8.81 8.04 8.3 9.04 8.83 ...\r\n $ pf_movement_domestic              : num [1:1458] 5 5 0 10 5 10 10 5 10 10 ...\r\n $ pf_movement_foreign               : num [1:1458] 10 5 5 10 5 10 10 5 10 5 ...\r\n $ pf_movement_women                 : num [1:1458] 5 5 10 10 10 10 10 5 NA 5 ...\r\n $ pf_movement                       : num [1:1458] 6.67 5 5 10 6.67 ...\r\n $ pf_religion_estop_establish       : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_religion_estop_operate         : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_religion_estop                 : num [1:1458] 10 5 10 7.5 5 10 10 2.5 NA 10 ...\r\n $ pf_religion_harassment            : num [1:1458] 9.57 6.87 8.9 9.04 8.58 ...\r\n $ pf_religion_restrictions          : num [1:1458] 8.01 2.96 7.46 6.85 5.09 ...\r\n $ pf_religion                       : num [1:1458] 9.19 4.94 8.79 7.8 6.22 ...\r\n $ pf_association_association        : num [1:1458] 10 5 2.5 7.5 7.5 10 10 2.5 NA 5 ...\r\n $ pf_association_assembly           : num [1:1458] 10 5 2.5 10 7.5 10 10 5 NA 0 ...\r\n $ pf_association_political_establish: num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_association_political_operate  : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_association_political          : num [1:1458] 10 5 2.5 5 5 10 10 2.5 NA 0 ...\r\n $ pf_association_prof_establish     : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_association_prof_operate       : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_association_prof               : num [1:1458] 10 5 5 7.5 5 10 10 2.5 NA 10 ...\r\n $ pf_association_sport_establish    : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_association_sport_operate      : num [1:1458] NA NA NA NA NA NA NA NA NA NA ...\r\n $ pf_association_sport              : num [1:1458] 10 5 7.5 7.5 7.5 10 10 2.5 NA 10 ...\r\n $ pf_association                    : num [1:1458] 10 5 4 7.5 6.5 10 10 3 NA 5 ...\r\n $ pf_expression_killed              : num [1:1458] 10 10 10 10 10 10 10 10 10 10 ...\r\n $ pf_expression_jailed              : num [1:1458] 10 10 10 10 10 ...\r\n $ pf_expression_influence           : num [1:1458] 5 2.67 2.67 5.67 3.33 ...\r\n $ pf_expression_control             : num [1:1458] 5.25 4 2.5 5.5 4.25 7.75 8 0.25 7.25 0.75 ...\r\n $ pf_expression_cable               : num [1:1458] 10 10 7.5 10 7.5 10 10 10 NA 7.5 ...\r\n $ pf_expression_newspapers          : num [1:1458] 10 7.5 5 10 7.5 10 10 0 NA 7.5 ...\r\n $ pf_expression_internet            : num [1:1458] 10 7.5 7.5 10 7.5 10 10 7.5 NA 2.5 ...\r\n $ pf_expression                     : num [1:1458] 8.61 7.38 6.45 8.74 7.15 ...\r\n $ pf_identity_legal                 : num [1:1458] 0 NA 10 10 7 7 10 0 NA NA ...\r\n $ pf_identity_parental_marriage     : num [1:1458] 10 0 10 10 10 10 10 10 10 0 ...\r\n $ pf_identity_parental_divorce      : num [1:1458] 10 5 10 10 10 10 10 10 10 0 ...\r\n $ pf_identity_parental              : num [1:1458] 10 2.5 10 10 10 10 10 10 10 0 ...\r\n $ pf_identity_sex_male              : num [1:1458] 10 0 0 10 10 10 10 10 10 10 ...\r\n $ pf_identity_sex_female            : num [1:1458] 10 0 0 10 10 10 10 10 10 10 ...\r\n $ pf_identity_sex                   : num [1:1458] 10 0 0 10 10 10 10 10 10 10 ...\r\n $ pf_identity_divorce               : num [1:1458] 5 0 10 10 5 10 10 5 NA 0 ...\r\n $ pf_identity                       : num [1:1458] 6.25 0.833 7.5 10 8 ...\r\n $ pf_score                          : num [1:1458] 7.6 5.28 6.11 8.1 6.91 ...\r\n $ pf_rank                           : num [1:1458] 57 147 117 42 84 11 8 131 64 114 ...\r\n $ ef_government_consumption         : num [1:1458] 8.23 2.15 7.6 5.34 7.26 ...\r\n $ ef_government_transfers           : num [1:1458] 7.51 7.82 8.89 6.05 7.75 ...\r\n $ ef_government_enterprises         : num [1:1458] 8 0 0 6 8 10 10 0 7 10 ...\r\n $ ef_government_tax_income          : num [1:1458] 9 7 10 7 5 5 4 9 10 10 ...\r\n $ ef_government_tax_payroll         : num [1:1458] 7 2 9 1 5 5 3 4 10 10 ...\r\n $ ef_government_tax                 : num [1:1458] 8 4.5 9.5 4 5 5 3.5 6.5 10 10 ...\r\n $ ef_government                     : num [1:1458] 7.94 3.62 6.5 5.35 7 ...\r\n $ ef_legal_judicial                 : num [1:1458] 2.67 4.19 1.84 3.69 3.87 ...\r\n $ ef_legal_courts                   : num [1:1458] 3.15 4.33 1.97 2.93 4.2 ...\r\n $ ef_legal_protection               : num [1:1458] 4.51 4.69 2.51 4.26 5.66 ...\r\n $ ef_legal_military                 : num [1:1458] 8.33 4.17 3.33 7.5 5.83 ...\r\n $ ef_legal_integrity                : num [1:1458] 4.17 5 4.17 3.33 5 ...\r\n $ ef_legal_enforcement              : num [1:1458] 4.39 4.51 2.3 3.63 5.2 ...\r\n $ ef_legal_restrictions             : num [1:1458] 6.49 6.63 5.46 6.86 9.8 ...\r\n $ ef_legal_police                   : num [1:1458] 6.93 6.14 3.02 3.39 5.71 ...\r\n $ ef_legal_crime                    : num [1:1458] 6.22 6.74 4.29 4.13 7.01 ...\r\n $ ef_legal_gender                   : num [1:1458] 0.949 0.821 0.846 0.769 1 ...\r\n $ ef_legal                          : num [1:1458] 5.07 4.69 2.96 3.9 5.81 ...\r\n $ ef_money_growth                   : num [1:1458] 8.99 6.96 9.39 5.23 9.08 ...\r\n $ ef_money_sd                       : num [1:1458] 9.48 8.34 4.99 5.22 9.26 ...\r\n $ ef_money_inflation                : num [1:1458] 9.74 8.72 3.05 2 9.75 ...\r\n $ ef_money_currency                 : num [1:1458] 10 5 5 10 10 10 10 5 0 10 ...\r\n $ ef_money                          : num [1:1458] 9.55 7.25 5.61 5.61 9.52 ...\r\n $ ef_trade_tariffs_revenue          : num [1:1458] 9.63 8.48 8.99 6.06 8.87 ...\r\n $ ef_trade_tariffs_mean             : num [1:1458] 9.24 6.22 7.72 7.26 8.76 9.5 8.96 8.2 3.36 9.06 ...\r\n $ ef_trade_tariffs_sd               : num [1:1458] 8.02 5.92 4.25 5.94 8.02 ...\r\n $ ef_trade_tariffs                  : num [1:1458] 8.96 6.87 6.99 6.42 8.55 ...\r\n $ ef_trade_regulatory_nontariff     : num [1:1458] 5.57 4.96 3.13 4.47 5.92 ...\r\n $ ef_trade_regulatory_compliance    : num [1:1458] 9.405 0 0.917 5.156 8.466 ...\r\n $ ef_trade_regulatory               : num [1:1458] 7.49 2.48 2.02 4.81 7.19 ...\r\n $ ef_trade_black                    : num [1:1458] 10 5.56 10 0 10 ...\r\n $ ef_trade_movement_foreign         : num [1:1458] 6.31 3.66 2.95 5.36 5.11 ...\r\n $ ef_trade_movement_capital         : num [1:1458] 4.615 0 3.077 0.769 5.385 ...\r\n $ ef_trade_movement_visit           : num [1:1458] 8.297 1.106 0.111 7.965 10 ...\r\n $ ef_trade_movement                 : num [1:1458] 6.41 1.59 2.04 4.7 6.83 ...\r\n $ ef_trade                          : num [1:1458] 8.21 4.13 5.26 3.98 8.14 ...\r\n  [list output truncated]\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   year = col_double(),\r\n  ..   ISO_code = col_character(),\r\n  ..   countries = col_character(),\r\n  ..   region = col_character(),\r\n  ..   pf_rol_procedural = col_double(),\r\n  ..   pf_rol_civil = col_double(),\r\n  ..   pf_rol_criminal = col_double(),\r\n  ..   pf_rol = col_double(),\r\n  ..   pf_ss_homicide = col_double(),\r\n  ..   pf_ss_disappearances_disap = col_double(),\r\n  ..   pf_ss_disappearances_violent = col_double(),\r\n  ..   pf_ss_disappearances_organized = col_double(),\r\n  ..   pf_ss_disappearances_fatalities = col_double(),\r\n  ..   pf_ss_disappearances_injuries = col_double(),\r\n  ..   pf_ss_disappearances = col_double(),\r\n  ..   pf_ss_women_fgm = col_double(),\r\n  ..   pf_ss_women_missing = col_double(),\r\n  ..   pf_ss_women_inheritance_widows = col_double(),\r\n  ..   pf_ss_women_inheritance_daughters = col_double(),\r\n  ..   pf_ss_women_inheritance = col_double(),\r\n  ..   pf_ss_women = col_double(),\r\n  ..   pf_ss = col_double(),\r\n  ..   pf_movement_domestic = col_double(),\r\n  ..   pf_movement_foreign = col_double(),\r\n  ..   pf_movement_women = col_double(),\r\n  ..   pf_movement = col_double(),\r\n  ..   pf_religion_estop_establish = col_double(),\r\n  ..   pf_religion_estop_operate = col_double(),\r\n  ..   pf_religion_estop = col_double(),\r\n  ..   pf_religion_harassment = col_double(),\r\n  ..   pf_religion_restrictions = col_double(),\r\n  ..   pf_religion = col_double(),\r\n  ..   pf_association_association = col_double(),\r\n  ..   pf_association_assembly = col_double(),\r\n  ..   pf_association_political_establish = col_double(),\r\n  ..   pf_association_political_operate = col_double(),\r\n  ..   pf_association_political = col_double(),\r\n  ..   pf_association_prof_establish = col_double(),\r\n  ..   pf_association_prof_operate = col_double(),\r\n  ..   pf_association_prof = col_double(),\r\n  ..   pf_association_sport_establish = col_double(),\r\n  ..   pf_association_sport_operate = col_double(),\r\n  ..   pf_association_sport = col_double(),\r\n  ..   pf_association = col_double(),\r\n  ..   pf_expression_killed = col_double(),\r\n  ..   pf_expression_jailed = col_double(),\r\n  ..   pf_expression_influence = col_double(),\r\n  ..   pf_expression_control = col_double(),\r\n  ..   pf_expression_cable = col_double(),\r\n  ..   pf_expression_newspapers = col_double(),\r\n  ..   pf_expression_internet = col_double(),\r\n  ..   pf_expression = col_double(),\r\n  ..   pf_identity_legal = col_double(),\r\n  ..   pf_identity_parental_marriage = col_double(),\r\n  ..   pf_identity_parental_divorce = col_double(),\r\n  ..   pf_identity_parental = col_double(),\r\n  ..   pf_identity_sex_male = col_double(),\r\n  ..   pf_identity_sex_female = col_double(),\r\n  ..   pf_identity_sex = col_double(),\r\n  ..   pf_identity_divorce = col_double(),\r\n  ..   pf_identity = col_double(),\r\n  ..   pf_score = col_double(),\r\n  ..   pf_rank = col_double(),\r\n  ..   ef_government_consumption = col_double(),\r\n  ..   ef_government_transfers = col_double(),\r\n  ..   ef_government_enterprises = col_double(),\r\n  ..   ef_government_tax_income = col_double(),\r\n  ..   ef_government_tax_payroll = col_double(),\r\n  ..   ef_government_tax = col_double(),\r\n  ..   ef_government = col_double(),\r\n  ..   ef_legal_judicial = col_double(),\r\n  ..   ef_legal_courts = col_double(),\r\n  ..   ef_legal_protection = col_double(),\r\n  ..   ef_legal_military = col_double(),\r\n  ..   ef_legal_integrity = col_double(),\r\n  ..   ef_legal_enforcement = col_double(),\r\n  ..   ef_legal_restrictions = col_double(),\r\n  ..   ef_legal_police = col_double(),\r\n  ..   ef_legal_crime = col_double(),\r\n  ..   ef_legal_gender = col_double(),\r\n  ..   ef_legal = col_double(),\r\n  ..   ef_money_growth = col_double(),\r\n  ..   ef_money_sd = col_double(),\r\n  ..   ef_money_inflation = col_double(),\r\n  ..   ef_money_currency = col_double(),\r\n  ..   ef_money = col_double(),\r\n  ..   ef_trade_tariffs_revenue = col_double(),\r\n  ..   ef_trade_tariffs_mean = col_double(),\r\n  ..   ef_trade_tariffs_sd = col_double(),\r\n  ..   ef_trade_tariffs = col_double(),\r\n  ..   ef_trade_regulatory_nontariff = col_double(),\r\n  ..   ef_trade_regulatory_compliance = col_double(),\r\n  ..   ef_trade_regulatory = col_double(),\r\n  ..   ef_trade_black = col_double(),\r\n  ..   ef_trade_movement_foreign = col_double(),\r\n  ..   ef_trade_movement_capital = col_double(),\r\n  ..   ef_trade_movement_visit = col_double(),\r\n  ..   ef_trade_movement = col_double(),\r\n  ..   ef_trade = col_double(),\r\n  ..   ef_regulation_credit_ownership = col_double(),\r\n  ..   ef_regulation_credit_private = col_double(),\r\n  ..   ef_regulation_credit_interest = col_double(),\r\n  ..   ef_regulation_credit = col_double(),\r\n  ..   ef_regulation_labor_minwage = col_double(),\r\n  ..   ef_regulation_labor_firing = col_double(),\r\n  ..   ef_regulation_labor_bargain = col_double(),\r\n  ..   ef_regulation_labor_hours = col_double(),\r\n  ..   ef_regulation_labor_dismissal = col_double(),\r\n  ..   ef_regulation_labor_conscription = col_double(),\r\n  ..   ef_regulation_labor = col_double(),\r\n  ..   ef_regulation_business_adm = col_double(),\r\n  ..   ef_regulation_business_bureaucracy = col_double(),\r\n  ..   ef_regulation_business_start = col_double(),\r\n  ..   ef_regulation_business_bribes = col_double(),\r\n  ..   ef_regulation_business_licensing = col_double(),\r\n  ..   ef_regulation_business_compliance = col_double(),\r\n  ..   ef_regulation_business = col_double(),\r\n  ..   ef_regulation = col_double(),\r\n  ..   ef_score = col_double(),\r\n  ..   ef_rank = col_double(),\r\n  ..   hf_score = col_double(),\r\n  ..   hf_rank = col_double(),\r\n  ..   hf_quartile = col_double()\r\n  .. )\r\n - attr(*, \"problems\")=<externalptr> \r\n\r\n\r\n\r\nselected_data <- hfi_data %>% \r\n  select(year, ISO_code, countries, region, pf_score, pf_expression_control)\r\n\r\n\r\n\r\n\r\n# hfi_data %>% \r\n # select(pf_score, pf_expression_control, hfi_data) %>% \r\n # ggpairs()\r\n\r\n\r\nNote that a warning message (really a list of warning messages) might display in your Console and likely under your R code chunk when you knit this report.\r\nIn R, warning messages are not necessarily a bad thing and you should read these to make sure you understand what it is informing you of.\r\nTo suppress warning messages from displaying after this specific R code chunk when you knit your report, add the follow inside the curly brackets ({r }) at the top of your R code chunk (notice the preceding comma): , warning=FALSE.\r\nSomewhat related… If you do not want all the messages {tidyverse} and {tidymodels} produce when you load them, you can add , message=FALSE to your load-packages R code chunk.\r\nAfter running the pairs-plot code, answer the following questions:\r\nFor each pair of variables, how would you describe the relationship graphically?\r\nDo any of the relationships look linear?\r\nAre there any interesting/odd features (outliers, non-linear patterns, etc.)?\r\nFor each pair of variables, how would you describe the relationship numerically?\r\nAre your two explanatory variables collinear (correlated)?\r\nEssentially, this means that adding more than one of these variables to the model would not add much value to the model.\r\nWe will talk more on this issue in Activity 4 (other considerations in regression models).\r\nThe multiple linear regression model\r\nYou will now fit the following model:\r\n\\[\r\ny = \\beta_0 + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 + \\varepsilon\r\n\\]\r\nIn the code chunk below titled mlr-model, replace “verbatim” with “r” just before the code chunk title.\r\nReplace explanatory, similarly to what you did in your pairs-plot R code chunk.\r\nRun your code chunk or knit your document.\r\n\r\n\r\n#fit the mlr model\r\nlm_spec <- linear_reg() %>%\r\n  set_mode(\"regression\") %>%\r\n  set_engine(\"lm\")\r\n\r\nlm_spec\r\n\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\n# mlr_mod <- lm_spec %>% \r\n# fit(pf_score ~ pf_expression_control + explanatory, data = hfi_2016)\r\n\r\n# model output\r\n# tidy(mlr_mod)\r\n\r\n\r\nAfter doing this, answer the following questions:\r\nUsing your output, write the complete estimated equation for this model.\r\nRemember in Activity 2 that this looked like:\r\n\\[\r\n\\hat{y} = b_0 + b_1 \\times x_1\r\n\\]\r\nwhere \\(b_0\\) and \\(b_1\\) were your model parameter estimates.\r\nNote that your model here will be different (and have more terms).\r\nFor each of the estimated parameters (the y-intercept and the slopes associated with each explanatory variable - three total), interpret these values in the context of this problem.\r\nThat is, what do they mean for a “non-data” person?\r\nChallenge: 3-D plots\r\nIn ISL, the authors provided a 3-D scatterplot with a plane that represents the estimated model.\r\nDo some internet sleuthing to minimally produce a 3-D scatterplot (you do not need to include the plane).\r\nIdeally, this would be something that plays nicely with (looks similar to) {ggplot2}.\r\nCreate a new R code chunk, with a descriptive name, and add your code to create this plot.\r\nAfter doing this, respond to the following prompt:\r\nCompare your 3-D scatterplot and the GGally::ggpairs output.\r\nComment on the strengths and weaknesses of these two visualizations.\r\nDo both display on GitHub when you push your work there?\r\nDay 2\r\nDuring Day 1, you fit a model with one quantitative response variable and two quantitative explanatory variables.\r\nNow we look at a model with one quantitative explanatory variable and one qualitative explanatory variable.\r\nWe will use the full 2016 dataset for this entire activity.\r\nFor the Mini-Competition next week, you will be instructed to use the train/test split process.\r\nFitting the overall model\r\nThis is similar to what we have already been doing - fitting our desired model.\r\nFor today’s activity, we will fit something like:\r\n\\[\r\ny = \\beta_0 + \\beta_1 \\times \\text{qualitative\\\\_variable} + \\beta_2 \\times \\text{quantitative\\\\_variable} + \\varepsilon\r\n\\]\r\nwhere \\(y\\), \\(\\text{qualitative\\\\_variable}\\), and \\(\\text{quantitative\\\\_variable}\\) are from hfi_2016.\r\nNote that the two explanatory variables can be entered in whatever order.\r\nTo help with interpretability, we will focus on qualitative predictor variables with only two levels.\r\nUnfortunately, none of the current chr variables have only two levels.\r\nFortunately, we can create our own.\r\nIn the code chunk below titled binary-pred, replace “verbatim” with “r” just before the code chunk title.\r\nRun your code chunk or knit your document.\r\n\r\n\r\nhfi_2016 <- hfi_2016 %>%\r\n  mutate(west_atlantic = if_else(\r\n    region %in% c(\"North America\", \"Latin America & the Caribbean\"),\r\n    \"No\",\r\n    \"Yes\"\r\n  ))\r\n\r\n\r\nWhat is happening in the above code? What new variable did we create? How do you know it is new? What values does it take when?\r\nIn the code chunk below titled qual-mlr, replace “verbatim” with “r” just before the code chunk title.\r\nRun your code chunk or knit your document.\r\n\r\n\r\n# review any visual patterns\r\nhfi_2016 %>% \r\n  select(pf_score, west_atlantic, pf_expression_control) %>% \r\n  ggpairs()\r\n\r\n\r\n#fit the mlr model\r\nlm_spec <- linear_reg() %>%\r\n  set_mode(\"regression\") %>%\r\n  set_engine(\"lm\")\r\n\r\nqual_mod <- lm_spec %>% \r\n  fit(pf_score ~ west_atlantic + pf_expression_control, data = hfi_2016)\r\n\r\n# model output\r\ntidy(qual_mod)\r\n\r\n# A tibble: 3 × 5\r\n  term                  estimate std.error statistic  p.value\r\n  <chr>                    <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)              4.38     0.213     20.5   1.57e-46\r\n2 west_atlanticYes        -0.102    0.167     -0.612 5.41e- 1\r\n3 pf_expression_control    0.540    0.0273    19.8   1.01e-44\r\n\r\nWhen looking at your ggpairs output, remember to ask yourself, “does it make sense to include all of these variables?”\r\nSpecifically, if you notice that the response variables are highly correlated (collinear), including both does not necessarily add much value as they are essentially saying the same thing.\r\nNote: There are more advanced methods to include the variability within a rater for our model - this is beyond STA 631.\r\nIf this sounds of interest to you, explore generalized estimating equations (GEE) or generalized linear mixed models (GLMM).\r\nHowever, there are often times when we choose to include variables in our model because it is important to us - for various reasons.\r\nRegardless, I encourage you to keep your readings of DF in mind - who will benefit by including this information; who will be hurt by including this information?\r\nAlso, when looking at your model (tidy) output, the term label for your qualitative explanatory variable look odd.\r\nAnswer the following questions:\r\nWhat is the label that R assigned to this explanatory variable term?\r\nWhat information is represented here?\r\nWhat information is missing here?\r\nYour are essentially fitting two models (or \\(k\\) models, where \\(k\\) is the number of levels in your qualitative variable).\r\nFrom your reading, you learned that R is creating an indicator variable (see p. 83).\r\nIf you have 3 levels in your qualitative variable, you would have 2 (3 - 1) indicator variables.\r\nIf you have \\(k\\) levels in your qualitative variable, you would have \\(k - 1\\) indicator variables.\r\nThe decision for R to call the indicator variable by one of your levels instead of the other has no deeper meaning.\r\nR simply codes the level that comes first alphabetically with a \\(0\\) for your indicator variable.\r\nYou can change this reference level of a categorical variable, which is the level that is coded as a 0, using the relevel function.\r\nUse ?relevel to learn more.\r\nWrite the estimated equation for your MLR model with a qualitative explanatory variable.\r\nNow, for each level of your qualitative variable, write the simplified equation of the estimated line for that level.\r\nNote that if your qualitative variable has two levels, you should have two simplified equations.\r\nThe interpretation of the coefficients (parameter estimates) in multiple regression is slightly different from that of simple regression.\r\nThe estimate for the indicator variable reflects how much more a group is expected to be if something has that quality, while holding all other variables constant.\r\nThe estimate for the quantitative variable reflects how much change in the response variable occurs due to a 1-unit increase in the quantitative variable, while holding all other variables constant.\r\nInterpret the parameter estimate for the reference level of your categorical variable in the context of your problem.\r\nPage 83 of the text can help here (or have me come chat with you).\r\nInterpret the parameter estimate for your quantitative variable in the context of your problem.\r\nChallenge: Multiple levels\r\nBelow, create a new R code chunk (with a descriptive name) that fits a new model with the same response (pf_score) and quantitative explanatory variable (pf_expression_control), but now use a qualitative variable with more than two levels (say, region) and obtain the tidy model output.\r\nHow does R appear to handle categorical variables with more than two levels?\r\nDay 3\r\nWe will explore a MLR model with an interaction between quantitative and qualitative explanatory variables as well as see some other methods to assess the fit of our model.\r\nFrom the modeling process we came up with as a class, we will now address the “series of important questions that we should consider when performing multiple linear regression” (ISL Section 3.2.2, p. 75):\r\nIs at least one of the \\(p\\) predictors \\(X_1\\), \\(X_2\\), \\(\\ldots\\), \\(X_p\\) useful in predicting the response \\(Y\\)?\r\nDo all the predictors help to explain \\(Y\\), or is only a subset of the predictors useful?\r\nHow well does the model fit the data?\r\nGiven a set of predictor values, what response value should we predict and how accurate is our prediction?\r\nNote that the text (ISLR) covers interactions between two quantitative explanatory variables as well.\r\nBy including an interaction term in our model, it may seem like we are relaxing the “additive assumption” a little.\r\nHowever, the additive assumption is about the coefficients (the \\(\\beta\\)s) and not the variables.\r\nFitting the overall model with \\(qualitative \\times quantitative\\) interaction\r\nRecall from Day 2 that you explored the model:\r\n\\[\r\ny = \\beta_0 + \\beta_1 \\times \\text{qualitative\\\\_variable} + \\beta_2 \\times \\text{quantitative\\\\_variable} + \\varepsilon\r\n\\]\r\nToday we will explore a similar model, except that also includes the interaction between your qualitative and quantitative explanatory variables.\r\nThat is,\r\n\\[\r\ny = \\beta_0 + \\beta_1 \\times \\text{qualitative\\\\_variable} + \\beta_2 \\times \\text{quantitative\\\\_variable} + \\beta_3 \\times ( \\text{qualitative\\\\_variable} \\times \\text{quantitative\\\\_variable}) + \\varepsilon\r\n\\]\r\nRun all previous code up to this point - you will need your prior dataset of just 2016 observations with the west_atlantic variable.\r\nIn the code chunk below titled int-mlr, replace “verbatim” with “r” just before the code chunk title.\r\nRun your code chunk or knit your document.\r\n\r\n\r\n# review any visual patterns\r\nhfi_2016 %>% \r\n  select(pf_score, west_atlantic, pf_expression_control) %>% \r\n  ggpairs()\r\n\r\n\r\n#fit the mlr model\r\nlm_spec <- linear_reg() %>%\r\n  set_mode(\"regression\") %>%\r\n  set_engine(\"lm\")\r\n\r\nint_mod <- lm_spec %>% \r\n  fit(pf_score ~ west_atlantic * pf_expression_control, data = hfi_2016)\r\n\r\n# model output\r\ntidy(int_mod)\r\n\r\n# A tibble: 4 × 5\r\n  term                           estimate std.error statistic  p.value\r\n  <chr>                             <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)                       5.72     0.459      12.5  2.76e-25\r\n2 west_atlanticYes                 -1.60     0.484      -3.30 1.18e- 3\r\n3 pf_expression_control             0.296    0.0789      3.75 2.45e- 4\r\n4 west_atlanticYes:pf_expressio…    0.275    0.0838      3.28 1.26e- 3\r\n\r\nNote that I shortened the model statement using qualitative * quantitative, but this can sometimes be confusing to read.\r\nAnother way to write the right-hand side of the equation is: qualitative + quantitative + qualitative * quantitative.\r\nAfter doing this, answer the following question:\r\nWhen viewing the tidy output, notice that the interaction term is listed as qualitativelevel:quantitative.\r\nReferring back to Day 2 with how R displays qualitative variables, interpret what this syntax means.\r\nUsing page 100 of ISLR as a reference, if needed, and your work from Day 2, write the simplified equation of the line corresponding to each level of your qualitative explanatory variable.\r\nFor two observations with similar values of the quantitative , which level tends to have higher values of the response variable?\r\nLike you did in Day 1, assess the fit of this model (no need to do any formal hypothesis testing - we will explore this next).\r\nHow does int_mod’s fit compare to mlr_mod?\r\nWhat did you use to compare these?\r\nWhy?\r\nRecall our brief discussion on how many disciplines are moving away from \\(p\\)-values in favor of other methods.\r\nWe will explore \\(p\\)-values these other methods later this semester, but we will practice our classical methods here.\r\nThis is known as an “overall \\(F\\) test” and the hypotheses are:\r\nThat (the null) no predictors are useful for the model (i.e., all slopes are equal to zero) versus the alternative that at least one predictor is useful for the model (i.e., at least one slope is not zero).\r\nOne way to check this is to build our null model (no predictors) and then compare this to our candidate model (int_mod).\r\nIn the code chunk below titled mod-comp, replace “verbatim” with “r” just before the code chunk title.\r\n\r\n\r\n# null model\r\n# null_mod <- lm_spec %>% \r\n# fit(response ~ 1, data = data)\r\n\r\n# anova(\r\n  # extract_fit_engine(int_mod),\r\n  # extract_fit_engine(null_mod)\r\n# )\r\n\r\n\r\nUsing your background knowledge of \\(F\\) tests, what is the \\(F\\) test statistic and \\(p\\)-value for this test?\r\nBased on an \\(\\alpha = 0.05\\) significant level, what should you conclude?\r\nPartial slope test - do all predictors help explain \\(y\\)?\r\nAssuming that your overall model is significant (at least one predictor is useful), we will continue on.\r\nContinue through these next tasks even if your overall model was not significant.\r\nWe could do a similar process to fit a new model while removing one explanatory variable at at time, and using anova to compare these models.\r\nHowever, the tidy output also helps here (the statistic and p.value columns).\r\nFor each slope, you are testing if that slope is zero (when including the other variables, the null) or if it is not zero (when including the other variables, the alternative).\r\nBecause the interaction term is a combination of the other two variables, we should assess the first.\r\nWhat is the \\(t\\) test statistic and \\(p\\)-value associated with this test?\r\nBased on an \\(\\alpha = 0.05\\) significant level, what should you conclude?\r\nIf your interaction term was not significant, you could consider removing it.\r\nNow look at your two non-interaction terms…\r\nWhat are the \\(t\\) test statistic and \\(p\\)-value associated with these tests?\r\nBased on an \\(\\alpha = 0.05\\) significant level, what should you conclude about these two predictors?\r\nYou would not need to do (21) if the interaction was significant.\r\nYou also should not remove a main variable (non-interaction variable) if the interaction variable remains in your model.\r\nResidual assessment - how well does the model fit the data?\r\nYou have already done this step in past activities by exploring your residuals (Activity 2).\r\nUsing your final model from Task 3, assess how well your model fits the data.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-16T16:57:14-04:00"
    },
    {
      "path": "myCV.html",
      "title": "myCV",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\r\nKARLMARX ODHIAMBO\r\nAllendale Charter Township, United States 49401\r\nPhone: +1 616-290-7124\r\nEmail: karlmarxorengo@gmail.com\r\nLinkedIn: Karlmarx Odhiambo\r\nGitHub: odhiambk\r\nEDUCATION\r\nMaster of Science: Data Science, Expected in 05/2025\r\nGrand Valley State University - Allendale, MI\r\nBachelor of Science: Actuarial Science, 09/2021\r\nUniversity of Nairobi - Nairobi, Kenya\r\nRanked in 10% of class with a 2nd Honors Upper Division\r\nGPA: 3.52\r\n\r\nPROFESSIONAL SUMMARY\r\nDedicated and results-driven Data Science graduate student with a strong foundation in statistical analysis, machine learning, and data visualization. Possessing a solid understanding of programming languages such as Python and R, coupled with hands-on experience in leveraging advanced analytics to derive meaningful insights from complex datasets. Proven ability to design and implement machine learning models for predictive and prescriptive analysis. Adept at utilizing data visualization tools to communicate findings effectively. Seeking opportunities to apply theoretical knowledge and practical skills in a dynamic and challenging professional environment, with a keen interest in contributing to innovative solutions and driving data-driven decision-making processes.\r\nSKILLS\r\nProgramming Languages: Python (NumPy, Pandas, Scikit-Learn), R\r\nData Analysis: Exploratory Data Analysis (EDA), Data Cleaning, Feature Engineering\r\nMachine Learning: Regression, Classification, Clustering, Model Evaluation\r\nTools: Jupyter Notebooks, Git, SQL\r\nData Visualization: Matplotlib, Seaborn\r\nStatistical Methods: Hypothesis Testing, Probability Distributions\r\nVersion Control: Git, GitHub\r\nDatabase Management: SQL, MongoDB\r\nEXPERIENCE\r\nDataAnalyzer Project, 10/2023 - 12/2023\r\nDeveloped a comprehensive data analysis tool in Python, incorporating NumPy, pandas, and Matplotlib libraries.\r\nImplemented functionalities for importing data, calculating descriptive statistics, and visualizing data using a user-friendly GUI with tkinter.\r\nEnsured robust error handling and modular design for enhanced usability and maintainability.\r\nApplied the tool to fetch and analyze real-world datasets, exemplifying practical application in data science tasks.\r\nDemonstrated proficiency in logging practices for effective troubleshooting and code maintenance.\r\nVolunteer Financial Analyst, 09/2021 - 03/2023\r\nCIC Insurance – Nairobi, Kenya\r\n- Achieved significant cost savings by identifying financial inefficiencies and implementing improved processes.\r\n- Streamlined financial reporting for better decision-making with clear, concise analysis and presentation.\r\n- Reduced risk exposure by conducting comprehensive credit analyses and recommending appropriate actions.\r\n- Assisted in M&A transactions, conducting valuation analyses to ensure favorable deal terms for the company.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-16T16:57:15-04:00"
    },
    {
      "path": "SLR.html",
      "title": "Simple Linear Regression",
      "author": [],
      "contents": "\r\nA typical modeling process\r\nThe process that we will use for today’s activity is:\r\nIdentify our research question(s),\r\nExplore (graphically and with numerical summaries) the variables of interest - both individually and in relationship to one another,\r\nFit a simple linear regression model to obtain and describe model estimates,\r\nAssess how “good” our model is, and\r\nPredict new values.\r\nWe will continue to update/tweak/adapt this process and you are encouraged to build your own process.\r\nBefore we begin, we set up our R session and introduce this activity’s data.\r\nDay 1\r\nThe setup\r\nWe will be using two packages from Posit (formerly RStudio): {tidyverse} and {tidymodels}.\r\nIf you would like to try the ISLR labs using these two packages instead of base R, Emil Hvitfeldt (of Posit) has put together a complementary online text.\r\nIn the Packages pane of RStudio (same area as Files), check to see if {tidyverse} and {tidymodels} are installed.\r\nBe sure to check both your User Library and System Library.\r\nIf either of these are not currently listed, type the following in your Console pane, replacing package_name with the appropriate name, and press Enter/Return afterwards.\r\n\r\n\r\n# Note: the \"eval = FALSE\" in the above line tells R not to evaluate this code\r\ninstall.packages(\"package_name\")\r\n\r\n\r\nOnce you have verified that both {tidyverse} and {tidymodels} are installed, load these packages in the R chunk below titled setup.\r\nThat is, type the following:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\n\r\n\r\nRun the setup code chunk and/or knit  icon your Rmd document to verify that no errors occur.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\n\r\n\r\nCheck in\r\nTest your GitHub skills by staging, committing, and pushing your changes to GitHub and verify that your changes have been added to your GitHub repository.\r\nThe data\r\nThe data we’re working with is from the OpenIntro site: https://www.openintro.org/data/csv/hfi.csv.\r\nHere is the “about” page: https://www.openintro.org/data/index.php?data=hfi.\r\nIn the R code chunk below titled load-data, you will type the code that reads in the above linked CSV file by doing the following:\r\n\r\n\r\n# Load data from the provided URL\r\nhfi <- readr::read_csv(\"https://www.openintro.org/data/csv/hfi.csv\")\r\n\r\n\r\nRather than downloading this file, uploading to RStudio, then reading it in, explore how to load this file directly from the provided URL with readr::read_csv ({readr} is part of {tidyverse}).\r\nAssign this data set into a data frame named hfi (short for “Human Freedom Index”).\r\nAfter doing this and viewing the loaded data, answer the following questions:\r\nWhat are the dimensions of the dataset? What does each row represent?\r\nThe dataset spans a lot of years.\r\nWe are only interested in data from year 2016.\r\nIn the R code chunk below titled hfi-2016, type the code that does the following:\r\nFilter the data hfi data frame for year 2016, and\r\nAssigns the result to a data frame named hfi_2016.\r\n\r\n\r\nhfi_2016 <- hfi %>%\r\n  filter(year == 2016)\r\n\r\n\r\n1. Identify our research question(s)\r\nThe research question is often defined by you (or your company, boss, etc.).\r\nToday’s research question/goal is to predict a country’s personal freedom score in 2016.\r\nFor this activity we want to explore the relationship between the personal freedom score, pf_score, and the political pressures and controls on media content index,pf_expression_control.\r\nSpecifically, we are going to use the political pressures and controls on media content index to predict a country’s personal freedom score in 2016.\r\n2. Explore the variables of interest\r\nAnswer the following questions (use your markdown skills) and complete the following tasks.\r\nWhat type of plot would you use to display the distribution of the personal freedom scores, pf_score? Would this be the same type of plot to display the distribution of the political pressures and controls on media content index, pf_expression_control?\r\nI would use a histogram to display the distribution of both plots.\r\nIn the R code chunk below titled univariable-plots, type the R code that displays this plot for pf_score.\r\nIn the R code chunk below titled univariable-plots, type the R code that displays this plot for pf_expression_control.\r\n\r\n\r\n# Displaying distribution plot for pf_score\r\nggplot(hfi_2016, aes(x = pf_score)) +\r\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\r\n  labs(title = \"Distribution of Personal Freedom Scores (pf_score)\",\r\n       x = \"Personal Freedom Score\",\r\n       y = \"Frequency\")\r\n\r\n\r\n# Displaying distribution plot for pf_expression_control\r\nggplot(hfi_2016, aes(x = pf_expression_control)) +\r\n  geom_histogram(binwidth = 1, fill = \"lightgreen\", color = \"black\", alpha = 0.7) +\r\n  labs(title = \"Distribution of Political Pressures and Controls on Media Content Index (pf_expression_control)\",\r\n       x = \"Political Pressures and Controls on Media Content Index\",\r\n       y = \"Frequency\")\r\n\r\n\r\n\r\nComment on each of these two distributions.\r\nBe sure to describe their centers, spread, shape, and any potential outliers.\r\nBoth distributions are unimodal (slightly skewed to the left).\r\nWhat type of plot would you use to display the relationship between the personal freedom score, pf_score, and the political pressures and controls on media content index,pf_expression_control?\r\nI’d use a ggplot to display the relationship between the two distributions.\r\nIn the R code chunk below titled relationship-plot, plot this relationship using the variable pf_expression_control as the predictor/explanatory variable.\r\n\r\n\r\n# Relationship plot between pf_score and pf_expression_control\r\nggplot(hfi_2016, aes(x = pf_expression_control, y = pf_score)) +\r\n  geom_point(color = \"darkorange\", alpha = 0.7) +\r\n  labs(title = \"Relationship between Personal Freedom Score (pf_score) and\\nPolitical Pressures and Controls on Media Content Index (pf_expression_control)\",\r\n       x = \"Political Pressures and Controls on Media Content Index\",\r\n       y = \"Personal Freedom Score\")\r\n\r\n\r\n\r\nDoes the relationship look linear? If you knew a country’s pf_expression_control, or its score out of 10, with 0 being the most, of political pressures and controls on media content, would you be comfortable using a linear model to predict the personal freedom score?\r\n**The relationship appears to be linear. Based on the plot, I’d be comfortable using a linear model to predict the personal freedom score\r\nChallenge\r\nFor each plot and using your {dplyr} skills, obtain the appropriate numerical summary statistics and provide more detailed descriptions of these plots.\r\nFor example, in (4) you were asked to comment on the center, spread, shape, and potential outliers.\r\nWhat measures could/should be used to describe these?\r\nFor each plot and the relationship between the variables, measures such as the mean, median, standard deviation, minimum, and maximum can be used to describe the center, spread, and potential outliers of the distribution.\r\nThe correlation coefficient provides a numerical summary of the relationship between two numerical variables in the case of a scatter plot.\r\nYou might not know of one for each of those terms.\r\nWhat numerical summary would you use to describe the relationship between two numerical variables?\r\n(hint: explore the cor function from Base R)\r\n\r\n\r\n# Summary statistics for pf_score\r\nsummary_pf_score <- hfi_2016 %>%\r\n  summarize(\r\n    mean_pf_score = mean(pf_score),\r\n    median_pf_score = median(pf_score),\r\n    sd_pf_score = sd(pf_score),\r\n    min_pf_score = min(pf_score),\r\n    max_pf_score = max(pf_score)\r\n  )\r\n\r\nsummary_pf_score\r\n\r\n# A tibble: 1 × 5\r\n  mean_pf_score median_pf_score sd_pf_score min_pf_score max_pf_score\r\n          <dbl>           <dbl>       <dbl>        <dbl>        <dbl>\r\n1          6.98            6.93        1.49         2.17         9.40\r\n\r\n# Summary statistics for pf_expression_control\r\nsummary_pf_expression_control <- hfi_2016 %>%\r\n  summarize(\r\n    mean_pf_expression_control = mean(pf_expression_control),\r\n    median_pf_expression_control = median(pf_expression_control),\r\n    sd_pf_expression_control = sd(pf_expression_control),\r\n    min_pf_expression_control = min(pf_expression_control),\r\n    max_pf_expression_control = max(pf_expression_control)\r\n  )\r\n\r\nsummary_pf_expression_control\r\n\r\n# A tibble: 1 × 5\r\n  mean_pf_expression_c…¹ median_pf_expression…² sd_pf_expression_con…³\r\n                   <dbl>                  <dbl>                  <dbl>\r\n1                   4.98                      5                   2.32\r\n# ℹ abbreviated names: ¹​mean_pf_expression_control,\r\n#   ²​median_pf_expression_control, ³​sd_pf_expression_control\r\n# ℹ 2 more variables: min_pf_expression_control <dbl>,\r\n#   max_pf_expression_control <dbl>\r\n\r\n# Correlation between pf_score and pf_expression_control\r\ncorrelation <- cor(hfi_2016$pf_score, hfi_2016$pf_expression_control)\r\ncorrelation\r\n\r\n[1] 0.8450646\r\n\r\n3. Fit a simple linear regression model\r\nRegardless of your response to (4), we will continue fitting a simple linear regression (SLR) model to these data.\r\nThe code that we will be using to fit statistical models in this course use {tidymodels} - an opinionated way to fit models in R - and this is likely new to most of you.\r\nI will provide you with example code when I do not think you should know what to do - i.e., anything {tidymodels} related.\r\nTo begin, we will create a {parsnip} specification for a linear model.\r\nIn the code chunk below titled parsnip-spec, replace “verbatim” with “r” just before the code chunk title.\r\n\r\n\r\nlm_spec <- linear_reg() %>%\r\n  set_mode(\"regression\") %>%\r\n  set_engine(\"lm\")\r\n\r\nlm_spec\r\n\r\nLinear Regression Model Specification (regression)\r\n\r\nComputational engine: lm \r\n\r\nNote that the set_mode(\"regression\") is really unnecessary/redundant as linear models (\"lm\") can only be regression models.\r\nIt is better to be explicit as we get comfortable with this new process.\r\nRemember that you can type ?function_name in the R Console to explore a function’s help documentation.\r\nThe above code also outputs the lm_spec output.\r\nThis code does not do any calculations by itself, but rather specifies what we plan to do.\r\nUsing this specification, we can now fit our model: \\(\\texttt{pf\\score} = \\beta_0 + \\beta_1 \\times \\texttt{pf\\_expression\\_control} + \\varepsilon\\).\r\nNote, the “$” portion in the previous sentence is LaTeX snytex which is a math scripting (and other scripting) language.\r\nI do not expect you to know this, but you will become more comfortable with this.\r\nLook at your knitted document to see how this syntax appears.\r\nIn the code chunk below titled fit-lm, replace “verbatim” with “r” just before the code chunk title.\r\n\r\n\r\nslr_mod <- lm_spec %>% \r\n  fit(pf_score ~ pf_expression_control, data = hfi_2016)\r\n\r\ntidy(slr_mod)\r\n\r\n# A tibble: 2 × 5\r\n  term                  estimate std.error statistic  p.value\r\n  <chr>                    <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)              4.28     0.149       28.8 4.23e-65\r\n2 pf_expression_control    0.542    0.0271      20.0 2.31e-45\r\n\r\nThe above code fits our SLR model, then provides a tidy parameter estimates table.\r\nUsing the tidy output, update the below formula with the estimated parameters. That is, replace “intercept” and “slope” with the appropriate values\r\n\\(\\hat{\\texttt{pf\\score}} = intercept + slope \\times \\texttt{pf\\_expression\\_control}\\)\r\n\r\n\r\n# Assuming model is the object resulting from your linear regression\r\nmodel <- lm(pf_score ~ pf_expression_control, data = hfi_2016)\r\ntidy_output <- tidy(model)\r\n\r\n# Extracting intercept and slope\r\nestimate_intercept <- tidy_output %>%\r\n  filter(term == \"(Intercept)\") %>%\r\n  pull(estimate)\r\n\r\nestimate_slope <- tidy_output %>%\r\n  filter(term == \"pf_expression_control\") %>%\r\n  pull(estimate)\r\n\r\n# Updated formula\r\nupdated_formula <- glue::glue(\"pf_score = {estimate_intercept} + {estimate_slope} * pf_expression_control\")\r\n\r\n# Print the updated formula\r\ncat(updated_formula)\r\n\r\npf_score = 4.28381533374574 + 0.541845207254726 * pf_expression_control\r\n\r\nInterpret each of the estimated parameters from (5) in the context of this research question. That is, what do these values represent?\r\nThe estimated intercept represents the predicted value of the personal freedom score when the political pressures and controls on media content index (pf_expression_control pf_expression_control) is zero.\r\nThe estimated slope represents the change in the predicted personal freedom score for a one-unit change in the political pressures and controls on media content index (pf_expression_control pf_expression_control).\r\nDay 2\r\nHopefully, you were able to interpret the SLR model parameter estimates (i.e., the y-intercept and slope) as follows:\r\n\r\nFor countries with a pf_expression_control of 0 (those with the largest amount of political pressure on media content), we expect their mean personal freedom score to be 4.28.\r\n\r\n\r\nFor every 1 unit increase in pf_expression_control (political pressure on media content index), we expect a country’s mean personal freedom score to increase 0.542 units.\r\n\r\n4. Assessing\r\n4.A: Assess with your Day 1 model\r\nTo assess our model fit, we can use \\(R^2\\) (the coefficient of determination), the proportion of variability in the response variable that is explained by the explanatory variable.\r\nWe use glance from {broom} (which is automatically loaded with {tidymodels} - {broom} is also where tidy is from) to access this information.\r\nIn the code chunk below titled glance-lm, replace “verbatim” with “r” just before the code chunk title.\r\n\r\n\r\nglance(slr_mod)\r\n\r\n# A tibble: 1 × 12\r\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\r\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\r\n1     0.714         0.712 0.799      400. 2.31e-45     1  -193.  391.\r\n# ℹ 4 more variables: BIC <dbl>, deviance <dbl>, df.residual <int>,\r\n#   nobs <int>\r\n\r\nAfter doing this and running the code, answer the following questions:\r\nWhat is the value of \\(R^2\\) for this model?\r\n\\(R^2\\) = 0.7141\r\nWhat does this value mean in the context of this model?\r\nThink about what would a “good” value of \\(R^2\\) would be?\r\nCan/should this value be “perfect”?\r\nIn the context of this model, an \\(R^2\\) value of 0.7141 is actually good. The closer the \\(R^2\\) value is to 1, the better.\r\n4.B: Assess with test/train\r\nYou previously fit a model and evaluated it using the exact same data.\r\nThis is a bit of circular reasoning and does not provide much information about the model’s performance.\r\nNow we will work through the test/train process of fitting and assessing a simple linear regression model.\r\nUsing the diamonds example provided to you in the Day 2 README, do the following\r\nCreate a new R code chunk and provide it with a descriptive tile (e.g., train-test).\r\nSet a seed.\r\nCreate an initial 80-20 split of the hfi_2016 dataset\r\nUsing your initial split R object, assign the two splits into a training R object and a testing R object.\r\n\r\n\r\n# Seed for reproducibility\r\nset.seed(123)\r\n\r\n# Initial 80-20 split of the hfi_2016 dataset\r\ninitial_split <- initial_split(hfi_2016, prop = 0.8)\r\n\r\n# Training R object and testing R object\r\ntraining_data <- training(initial_split)\r\ntesting_data <- testing(initial_split)\r\n\r\n\r\nNow, you will use your training dataset to fit a SLR model.\r\nIn the code chunk below titled train-fit-lm, replace “verbatim” with “r” just before the code chunk title and update the data set to your training R object you just created.\r\n\r\n\r\nslr_train <- lm_spec %>% \r\n  fit(pf_score ~ pf_expression_control, data = training_data)\r\n\r\ntidy(slr_train)\r\n\r\n# A tibble: 2 × 5\r\n  term                  estimate std.error statistic  p.value\r\n  <chr>                    <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)              4.32     0.166       26.1 7.85e-53\r\n2 pf_expression_control    0.536    0.0299      17.9 1.41e-36\r\n\r\nNotice that you can reuse the lm_spec specification because we are still doing a linear model.\r\nUsing the tidy output, update the below formula with the estimated parameters. That is, replace “intercept” and “slope” with the appropriate values\r\n\\(\\hat{\\texttt{pf\\score}} = intercept + slope \\times \\texttt{pf\\_expression\\_control}\\)\r\n\r\n\r\n# Extracting estimated parameters from tidy output\r\nestimate_intercept_train <- tidy(slr_train) %>%\r\n  filter(term == \"(Intercept)\") %>%\r\n  pull(estimate)\r\n\r\nestimate_slope_train <- tidy(slr_train) %>%\r\n  filter(term == \"pf_expression_control\") %>%\r\n  pull(estimate)\r\n\r\n# Updated formula\r\nupdated_formula_train <- glue::glue(\"pf_score = {estimate_intercept_train} + {estimate_slope_train} * pf_expression_control\")\r\n\r\n# Print the updated formula\r\ncat(updated_formula_train)\r\n\r\npf_score = 4.32097988300367 + 0.535622418099926 * pf_expression_control\r\n\r\nInterpret each of the estimated parameters from (10) in the context of this research question. That is, what do these values represent?\r\nNow we will assess using the testing data set.\r\nIn the code chunk below titled train-fit-lm, replace “verbatim” with “r” just before the code chunk title and update data_test to whatever R object you assigned your testing data to above.\r\n\r\n\r\ntest_aug <- augment(slr_train, new_data = testing_data)\r\ntest_aug\r\n\r\n# A tibble: 33 × 125\r\n   .pred .resid  year ISO_code countries  region     pf_rol_procedural\r\n   <dbl>  <dbl> <dbl> <chr>    <chr>      <chr>                  <dbl>\r\n 1  6.46 -1.18   2016 DZA      Algeria    Middle Ea…             NA   \r\n 2  5.66  0.451  2016 AGO      Angola     Sub-Sahar…             NA   \r\n 3  4.72  1.41   2016 BHR      Bahrain    Middle Ea…             NA   \r\n 4  7.94 -0.506  2016 BLZ      Belize     Latin Ame…              4.75\r\n 5  6.60  0.609  2016 BOL      Bolivia    Latin Ame…              3.70\r\n 6  7.13 -0.257  2016 BWA      Botswana   Sub-Sahar…              5.33\r\n 7  8.74  0.412  2016 CAN      Canada     North Ame…              8.62\r\n 8  8.47 -0.485  2016 CPV      Cape Verde Sub-Sahar…             NA   \r\n 9  5.12  0.226  2016 CHN      China      East Asia               3.95\r\n10  5.12 -1.23   2016 EGY      Egypt      Middle Ea…              2.95\r\n# ℹ 23 more rows\r\n# ℹ 118 more variables: pf_rol_civil <dbl>, pf_rol_criminal <dbl>,\r\n#   pf_rol <dbl>, pf_ss_homicide <dbl>,\r\n#   pf_ss_disappearances_disap <dbl>,\r\n#   pf_ss_disappearances_violent <dbl>,\r\n#   pf_ss_disappearances_organized <dbl>,\r\n#   pf_ss_disappearances_fatalities <dbl>, …\r\n\r\nThis takes your SLR model and applies it to your testing data.\r\nCheck in\r\nLook at the various information produced by this code.\r\nCan you identify what each column represents?\r\nThe .fitted column in this output can also be obtained by using predict (i.e., predict(slr_fit, new_data = data_test))\r\nNow, using your responses to (7) and (8) as an example, assess how well your model fits your testing data. Compare your results here to your results from your Day 1 of this activity. Did this model perform any differently?\r\nModel diagnostics\r\nTo assess whether the linear model is reliable, we should check for (1) linearity, (2) nearly normal residuals, and (3) constant variability.\r\nNote that the normal residuals is not really necessary for all models (sometimes we simply want to describe a relationship for the data that we have or population-level data, where statistical inference is not appropriate/necessary).\r\nIn order to do these checks we need access to the fitted (predicted) values and the residuals.\r\nWe can use broom::augment to calculate these.\r\nIn the code chunk below titled augment, replace “verbatim” with “r” just before the code chunk title and update data_test to whatever R object you assigned your testing data to above.\r\n\r\n\r\n# Check column names in test_aug\r\ncolnames(test_aug)\r\n\r\n  [1] \".pred\"                             \r\n  [2] \".resid\"                            \r\n  [3] \"year\"                              \r\n  [4] \"ISO_code\"                          \r\n  [5] \"countries\"                         \r\n  [6] \"region\"                            \r\n  [7] \"pf_rol_procedural\"                 \r\n  [8] \"pf_rol_civil\"                      \r\n  [9] \"pf_rol_criminal\"                   \r\n [10] \"pf_rol\"                            \r\n [11] \"pf_ss_homicide\"                    \r\n [12] \"pf_ss_disappearances_disap\"        \r\n [13] \"pf_ss_disappearances_violent\"      \r\n [14] \"pf_ss_disappearances_organized\"    \r\n [15] \"pf_ss_disappearances_fatalities\"   \r\n [16] \"pf_ss_disappearances_injuries\"     \r\n [17] \"pf_ss_disappearances\"              \r\n [18] \"pf_ss_women_fgm\"                   \r\n [19] \"pf_ss_women_missing\"               \r\n [20] \"pf_ss_women_inheritance_widows\"    \r\n [21] \"pf_ss_women_inheritance_daughters\" \r\n [22] \"pf_ss_women_inheritance\"           \r\n [23] \"pf_ss_women\"                       \r\n [24] \"pf_ss\"                             \r\n [25] \"pf_movement_domestic\"              \r\n [26] \"pf_movement_foreign\"               \r\n [27] \"pf_movement_women\"                 \r\n [28] \"pf_movement\"                       \r\n [29] \"pf_religion_estop_establish\"       \r\n [30] \"pf_religion_estop_operate\"         \r\n [31] \"pf_religion_estop\"                 \r\n [32] \"pf_religion_harassment\"            \r\n [33] \"pf_religion_restrictions\"          \r\n [34] \"pf_religion\"                       \r\n [35] \"pf_association_association\"        \r\n [36] \"pf_association_assembly\"           \r\n [37] \"pf_association_political_establish\"\r\n [38] \"pf_association_political_operate\"  \r\n [39] \"pf_association_political\"          \r\n [40] \"pf_association_prof_establish\"     \r\n [41] \"pf_association_prof_operate\"       \r\n [42] \"pf_association_prof\"               \r\n [43] \"pf_association_sport_establish\"    \r\n [44] \"pf_association_sport_operate\"      \r\n [45] \"pf_association_sport\"              \r\n [46] \"pf_association\"                    \r\n [47] \"pf_expression_killed\"              \r\n [48] \"pf_expression_jailed\"              \r\n [49] \"pf_expression_influence\"           \r\n [50] \"pf_expression_control\"             \r\n [51] \"pf_expression_cable\"               \r\n [52] \"pf_expression_newspapers\"          \r\n [53] \"pf_expression_internet\"            \r\n [54] \"pf_expression\"                     \r\n [55] \"pf_identity_legal\"                 \r\n [56] \"pf_identity_parental_marriage\"     \r\n [57] \"pf_identity_parental_divorce\"      \r\n [58] \"pf_identity_parental\"              \r\n [59] \"pf_identity_sex_male\"              \r\n [60] \"pf_identity_sex_female\"            \r\n [61] \"pf_identity_sex\"                   \r\n [62] \"pf_identity_divorce\"               \r\n [63] \"pf_identity\"                       \r\n [64] \"pf_score\"                          \r\n [65] \"pf_rank\"                           \r\n [66] \"ef_government_consumption\"         \r\n [67] \"ef_government_transfers\"           \r\n [68] \"ef_government_enterprises\"         \r\n [69] \"ef_government_tax_income\"          \r\n [70] \"ef_government_tax_payroll\"         \r\n [71] \"ef_government_tax\"                 \r\n [72] \"ef_government\"                     \r\n [73] \"ef_legal_judicial\"                 \r\n [74] \"ef_legal_courts\"                   \r\n [75] \"ef_legal_protection\"               \r\n [76] \"ef_legal_military\"                 \r\n [77] \"ef_legal_integrity\"                \r\n [78] \"ef_legal_enforcement\"              \r\n [79] \"ef_legal_restrictions\"             \r\n [80] \"ef_legal_police\"                   \r\n [81] \"ef_legal_crime\"                    \r\n [82] \"ef_legal_gender\"                   \r\n [83] \"ef_legal\"                          \r\n [84] \"ef_money_growth\"                   \r\n [85] \"ef_money_sd\"                       \r\n [86] \"ef_money_inflation\"                \r\n [87] \"ef_money_currency\"                 \r\n [88] \"ef_money\"                          \r\n [89] \"ef_trade_tariffs_revenue\"          \r\n [90] \"ef_trade_tariffs_mean\"             \r\n [91] \"ef_trade_tariffs_sd\"               \r\n [92] \"ef_trade_tariffs\"                  \r\n [93] \"ef_trade_regulatory_nontariff\"     \r\n [94] \"ef_trade_regulatory_compliance\"    \r\n [95] \"ef_trade_regulatory\"               \r\n [96] \"ef_trade_black\"                    \r\n [97] \"ef_trade_movement_foreign\"         \r\n [98] \"ef_trade_movement_capital\"         \r\n [99] \"ef_trade_movement_visit\"           \r\n[100] \"ef_trade_movement\"                 \r\n[101] \"ef_trade\"                          \r\n[102] \"ef_regulation_credit_ownership\"    \r\n[103] \"ef_regulation_credit_private\"      \r\n[104] \"ef_regulation_credit_interest\"     \r\n[105] \"ef_regulation_credit\"              \r\n[106] \"ef_regulation_labor_minwage\"       \r\n[107] \"ef_regulation_labor_firing\"        \r\n[108] \"ef_regulation_labor_bargain\"       \r\n[109] \"ef_regulation_labor_hours\"         \r\n[110] \"ef_regulation_labor_dismissal\"     \r\n[111] \"ef_regulation_labor_conscription\"  \r\n[112] \"ef_regulation_labor\"               \r\n[113] \"ef_regulation_business_adm\"        \r\n[114] \"ef_regulation_business_bureaucracy\"\r\n[115] \"ef_regulation_business_start\"      \r\n[116] \"ef_regulation_business_bribes\"     \r\n[117] \"ef_regulation_business_licensing\"  \r\n[118] \"ef_regulation_business_compliance\" \r\n[119] \"ef_regulation_business\"            \r\n[120] \"ef_regulation\"                     \r\n[121] \"ef_score\"                          \r\n[122] \"ef_rank\"                           \r\n[123] \"hf_score\"                          \r\n[124] \"hf_rank\"                           \r\n[125] \"hf_quartile\"                       \r\n\r\n\r\n\r\n# Plotting residuals vs. fitted values\r\nggplot(test_aug, aes(x = .pred, y = .resid)) +\r\n  geom_point(color = \"steelblue\", alpha = 0.7) +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"darkred\") +\r\n  labs(title = \"Residuals vs. Fitted Values\",\r\n       x = \"Fitted Values\",\r\n       y = \"Residuals\")\r\n\r\n\r\n# Checking for nearly normal residuals with a histogram\r\nggplot(test_aug, aes(x = .resid)) +\r\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\r\n  labs(title = \"Histogram of Residuals\",\r\n       x = \"Residuals\",\r\n       y = \"Frequency\")\r\n\r\n\r\n# Checking for constant variability with a plot of residuals vs. fitted values\r\nggplot(test_aug, aes(x = .pred, y = sqrt(abs(.resid)))) +\r\n  geom_point(color = \"darkorange\", alpha = 0.7) +\r\n  geom_smooth(se = FALSE, color = \"darkred\") +\r\n  labs(title = \"Residuals vs. Fitted Values (Square Root of Absolute Residuals)\",\r\n       x = \"Fitted Values\",\r\n       y = \"Square Root of Absolute Residuals\")\r\n\r\n\r\n\r\nLinearity: You already checked if the relationship between pf_score and pf_expression_control is linear using a scatterplot.\r\nWe should also verify this condition with a plot of the residuals vs. fitted (predicted) values.\r\nIn the code chunk below titled fitted-residual, replace “verbatim” with “r” just before the code chunk title.\r\n\r\n\r\n# Plotting residuals vs. fitted values for the training set\r\nggplot(data = test_aug, aes(x = .pred, y = .resid)) +\r\n  geom_point() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\r\n  xlab(\"Fitted values\") +\r\n  ylab(\"Residuals\")\r\n\r\n\r\n\r\nNotice here that train_aug can also serve as a data set because stored within it are the fitted values (\\(\\hat{y}\\)) and the residuals.\r\nAlso note that we are getting fancy with the code here.\r\nAfter creating the scatterplot on the first layer (first line of code), we overlay a red horizontal dashed line at \\(y = 0\\) (to help us check whether the residuals are distributed around 0), and we also rename the axis labels to be more informative.\r\nAnswer the following question:\r\nIs there any apparent pattern in the residuals plot? What does this indicate about the linearity of the relationship between the two variables?\r\nNearly normal residuals: To check this condition, we can look at a histogram of the residuals.\r\nIn the code chunk below titled residual-histogram, replace “verbatim” with “r” just before the code chunk title.\r\n\r\n\r\nggplot(data = test_aug, aes(x = .resid)) +\r\n  geom_histogram(binwidth = 0.25) +\r\n  xlab(\"Residuals\")\r\n\r\n\r\n\r\nAnswer the following question:\r\nBased on the histogram, does the nearly normal residuals condition appear to be violated? Why or why not?\r\nConstant variability:\r\nBased on the residuals vs. fitted plot, does the constant variability condition appear to be violated? Why or why not?\r\nAttribution\r\nThis document is based on labs from OpenIntro.\r\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\r\n\r\n\r\n\r\n",
      "last_modified": "2024-04-16T16:57:31-04:00"
    }
  ],
  "collections": []
}
